{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nagasri37/NLP/blob/main/Assignment3_447.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "19k41A0447\n",
        "\n",
        "NLP ASSIGNMENT - 3\n",
        "\n",
        "A paragraph is a series of sentences that are organized and coherent, and are all related to a single topic. Almost every piece of writing you do that is longer than a few sentences should be organized into paragraphs. This is because paragraphs show a reader where the subdivisions of an essay begin and end, and thus help the reader see the organization of the essay and grasp its main points.\n",
        "\n",
        "Paragraphs can contain many different kinds of information. A paragraph could contain a series of brief examples or a single long illustration of a general point. It might describe a place, character, or process; narrate a series of events; compare or contrast two or more things; classify items into categories; or describe causes and effects. Regardless of the kind of information they contain, all paragraphs share certain characteristics. One of the most important of these is a topic sentence.\n",
        "\n",
        "QUESTIONS:\n",
        "\n",
        "1.Convert the above paragraph into vectors using:\n",
        "\n",
        "i) Word2vec\n",
        "\n",
        "ii) USE\n",
        "\n",
        "iii) ELMO\n",
        "\n",
        "iv) GP2\n",
        "\n",
        "v) Sentence-BERT\n",
        "\n",
        "2.Find named entities (NER) for the above paragraph?\n",
        "\n",
        "3.Find similar sentences (repeated sentences) from the above paragraph? (Cosine Similarity, use BERT to encode)\n",
        "\n",
        "4.Explain POS tagging with HMM? Tag POS for the above paragraph?"
      ],
      "metadata": {
        "id": "EuwJPlLqppRw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "para = '''A paragraph is a series of sentences that are organized and coherent, and are all related to a single topic. Almost every piece of writing you do that is longer than a few sentences should be organized into paragraphs. This is because paragraphs show a reader where the subdivisions of an essay begin and end, and thus help the reader see the organization of the essay and grasp its main points.\n",
        "Paragraphs can contain many different kinds of information. A paragraph could contain a series of brief examples or a single long illustration of a general point. It might describe a place, character, or process; narrate a series of events; compare or contrast two or more things; classify items into categories; or describe causes and effects. Regardless of the kind of information they contain, all paragraphs share certain characteristics. One of the most important of these is a topic sentence.\n",
        "'''\n",
        "para"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "id": "Y93FwdS0qR8t",
        "outputId": "de5cc139-4c3f-45e0-a4f5-3e00966df123"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'A paragraph is a series of sentences that are organized and coherent, and are all related to a single topic. Almost every piece of writing you do that is longer than a few sentences should be organized into paragraphs. This is because paragraphs show a reader where the subdivisions of an essay begin and end, and thus help the reader see the organization of the essay and grasp its main points.\\nParagraphs can contain many different kinds of information. A paragraph could contain a series of brief examples or a single long illustration of a general point. It might describe a place, character, or process; narrate a series of events; compare or contrast two or more things; classify items into categories; or describe causes and effects. Regardless of the kind of information they contain, all paragraphs share certain characteristics. One of the most important of these is a topic sentence.\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1.Converting Paragraph into vectors"
      ],
      "metadata": {
        "id": "oqW252k4qFWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# importing required packages\n",
        "\n",
        "import numpy as np\n",
        "import re\n",
        "import nltk\n",
        "import gensim\n",
        "from nltk.corpus import stopwords\n",
        "from gensim.models import word2vec\n",
        "from nltk.tokenize import PunktSentenceTokenizer\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hg-c49anqGxg",
        "outputId": "f9a5029b-441f-4cc1-fdaf-b8d6a979ad43"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#converting paragraph to sentences\n",
        "\n",
        "def essay_to_sentences(paragraph):\n",
        "    tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
        "    raw_sentences = tokenizer.tokenize(paragraph.strip())\n",
        "    sentences = []\n",
        "    for raw_sentence in raw_sentences:\n",
        "        if len(raw_sentence) > 0:\n",
        "            sentences.append((raw_sentence))\n",
        "    return sentences\n",
        "\n",
        "sentences=essay_to_sentences(para)\n",
        "\n",
        "sentences"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UFQDOPzRjNxu",
        "outputId": "3a0a5195-29b0-474e-bcc3-7b58ec112ae9"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['A paragraph is a series of sentences that are organized and coherent, and are all related to a single topic.',\n",
              " 'Almost every piece of writing you do that is longer than a few sentences should be organized into paragraphs.',\n",
              " 'This is because paragraphs show a reader where the subdivisions of an essay begin and end, and thus help the reader see the organization of the essay and grasp its main points.',\n",
              " 'Paragraphs can contain many different kinds of information.',\n",
              " 'A paragraph could contain a series of brief examples or a single long illustration of a general point.',\n",
              " 'It might describe a place, character, or process; narrate a series of events; compare or contrast two or more things; classify items into categories; or describe causes and effects.',\n",
              " 'Regardless of the kind of information they contain, all paragraphs share certain characteristics.',\n",
              " 'One of the most important of these is a topic sentence.']"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# i) Word2vec : "
      ],
      "metadata": {
        "id": "YLA_O9HQjXsF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# initializing the word2vec model\n",
        "#converting words to vectors and removing stop words\n",
        "\n",
        "wordvecs=[nltk.word_tokenize(s) for s in sentences]\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "stops_words=list(set(stopwords.words(\"english\")))\n",
        "\n",
        "for i in wordvecs:\n",
        "  for j in i:\n",
        "    if j in stops_words:\n",
        "      i.remove(j)\n",
        "    elif len(j)==1:\n",
        "      i.remove(j)\n",
        "\n",
        "model=gensim.models.Word2Vec(wordvecs,min_count=1)\n",
        "\n",
        "\n",
        "model.wv['organized']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VJsw6tc-jgI9",
        "outputId": "601c4e35-1c35-42cb-f996-3221b297d91e"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "WARNING:gensim.models.base_any2vec:under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-2.8380468e-03, -1.0103624e-03, -2.6428183e-03, -3.3719379e-03,\n",
              "       -8.1203121e-04, -3.1851130e-04,  4.5655789e-03,  2.3558796e-03,\n",
              "       -2.0323561e-03,  3.6239177e-03, -3.3626717e-03,  4.8801461e-03,\n",
              "       -2.7564031e-03, -3.1450042e-03, -5.1713554e-04,  2.8887531e-03,\n",
              "        3.8413669e-03, -1.3470713e-03,  3.6644177e-03,  4.5470088e-03,\n",
              "        4.2031840e-03,  3.8692106e-03, -4.0178974e-03,  2.8786724e-03,\n",
              "        4.1479361e-03,  8.0391828e-06,  3.6386570e-03, -8.2117255e-04,\n",
              "       -3.2077995e-03, -4.0740157e-03, -3.2505700e-03,  3.1322627e-03,\n",
              "       -9.5872750e-04, -1.9009298e-03, -6.4103532e-04, -6.9437345e-05,\n",
              "        4.6171756e-03, -3.9240993e-03, -4.9762242e-04, -6.7185028e-04,\n",
              "       -3.0393093e-03,  2.2592529e-04, -5.4711354e-04,  6.9686142e-04,\n",
              "       -5.5932265e-04, -6.3741440e-04, -2.4991906e-03,  1.9013438e-03,\n",
              "        3.6546672e-03,  3.6830907e-03,  2.3322680e-03,  1.8007801e-03,\n",
              "       -4.7037308e-03,  4.5330995e-03,  3.9327741e-03,  1.8274417e-03,\n",
              "       -3.8737501e-03,  3.4413587e-03,  2.2651453e-03,  1.5475840e-03,\n",
              "       -1.3931318e-03, -1.9039013e-03, -7.3749776e-04,  3.9938325e-03,\n",
              "       -3.1657643e-03, -1.3519296e-03,  4.1224100e-03, -2.3148821e-03,\n",
              "        2.0094202e-03,  1.5464032e-03,  4.0354044e-03, -9.2960557e-04,\n",
              "        4.3639671e-03,  2.3391563e-03, -9.7096653e-04, -2.2100820e-03,\n",
              "       -1.0316854e-03,  2.1877752e-03,  4.6921666e-03, -1.8243747e-03,\n",
              "        3.1957461e-03, -1.7752858e-03, -2.1553016e-03,  3.9047094e-03,\n",
              "       -3.5939903e-03, -4.4763046e-03, -2.7060572e-03,  4.0059113e-03,\n",
              "       -1.2451097e-03, -7.9409330e-04, -2.4713294e-03,  2.6999044e-03,\n",
              "        3.1447080e-03, -2.2890193e-03,  4.9234447e-03,  3.4361426e-03,\n",
              "       -2.5968482e-03,  3.9875787e-03, -3.3158003e-03, -4.5842687e-03],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ii. USE:"
      ],
      "metadata": {
        "id": "4vatk3IKkta_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# importing the USE Model\n",
        "\n",
        "import tensorflow_hub as hub\n",
        "vect=hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")\n",
        "res_vectors=vect(sentences)\n",
        "print(res_vectors)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vFGcGXiRkySm",
        "outputId": "953e04c9-de26-438c-9116-d2dab8163b1f"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tensor(\"StatefulPartitionedCall_3:0\", shape=(None, 512), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"shape= \",res_vectors[0].shape)\n",
        "print(\"The sentence: \",sentences[0],\"\\n is converted as : \\n{}\".format(res_vectors[0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EqsVjZdpk8KW",
        "outputId": "89cd7698-095c-48d1-f75d-fb7df321f21b"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape=  (512,)\n",
            "The sentence:  A paragraph is a series of sentences that are organized and coherent, and are all related to a single topic. \n",
            " is converted as : \n",
            "Tensor(\"strided_slice_7:0\", shape=(512,), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# iii. ELMO"
      ],
      "metadata": {
        "id": "Wo7UaKNGlAzX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# importing the ELMO Model\n",
        "\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_eager_execution()\n",
        "elmo=hub.Module(\"https://tfhub.dev/google/elmo/3\",trainable=True)\n",
        "embeddings=elmo(\n",
        "    sentences,\n",
        "    signature=\"default\",\n",
        "    as_dict=True)[\"elmo\"]\n",
        "init=tf.initialize_all_variables()\n",
        "sess=tf.Session()\n",
        "sess.run(init)\n",
        "print(\"\\n\\n\")\n",
        "print(sess.run(embeddings[0]))\n",
        "print(\"shape=\",embeddings[0].shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WxXJOUhslCz4",
        "outputId": "3088a7e4-4af9-48ac-9edf-32cc7b41228b"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\n",
            "[[ 0.29287004 -0.14378013 -0.32574052 ... -0.39559263 -0.35758853\n",
            "  -0.03588088]\n",
            " [-0.59441584  0.09640743  0.50537694 ...  0.22031914  0.269769\n",
            "   0.46307266]\n",
            " [-0.1708326  -0.18744111 -0.27626696 ... -0.67550904  0.25389987\n",
            "   0.6540271 ]\n",
            " ...\n",
            " [-0.0284084  -0.04353216  0.04130162 ...  0.02583168 -0.01429836\n",
            "  -0.01650422]\n",
            " [-0.0284084  -0.04353216  0.04130162 ...  0.02583168 -0.01429836\n",
            "  -0.01650422]\n",
            " [-0.0284084  -0.04353216  0.04130162 ...  0.02583168 -0.01429836\n",
            "  -0.01650422]]\n",
            "shape= (32, 1024)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# iv. GP2"
      ],
      "metadata": {
        "id": "Vv7iHwhPleg_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RXBIehyUlR6W",
        "outputId": "02fea133-037b-43c7-aeba-e19eaa27f052"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.23.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (5.0.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.13.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.9.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.9.24)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#importing the GP2 Model\n",
        "import transformers\n",
        "gptokenizer=transformers.GPT2Tokenizer.from_pretrained('gpt2-large')\n",
        "model=transformers.GPT2LMHeadModel.from_pretrained('gpt2-large')\n",
        "res_vectors=gptokenizer.encode(para,add_special_tokens=False,return_tensors=\"pt\")\n",
        "print(\"shape=\",res_vectors.shape)\n",
        "res_vectors"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c5PtV4HIlIin",
        "outputId": "26737b07-adad-430b-88cd-be9dc91ebdd6"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape= torch.Size([1, 173])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[   32,  7322,   318,   257,  2168,   286, 13439,   326,   389,  8389,\n",
              "           290, 24870,    11,   290,   389,   477,  3519,   284,   257,  2060,\n",
              "          7243,    13, 16699,   790,  3704,   286,  3597,   345,   466,   326,\n",
              "           318,  2392,   621,   257,  1178, 13439,   815,   307,  8389,   656,\n",
              "         23549,    13,   770,   318,   780, 23549,   905,   257,  9173,   810,\n",
              "           262, 45944,  3279,   286,   281, 14268,  2221,   290,   886,    11,\n",
              "           290,  4145,  1037,   262,  9173,   766,   262,  4009,   286,   262,\n",
              "         14268,   290, 13180,   663,  1388,  2173,    13,   198, 10044,  6111,\n",
              "            82,   460,  3994,   867,  1180,  6982,   286,  1321,    13,   317,\n",
              "          7322,   714,  3994,   257,  2168,   286,  4506,  6096,   393,   257,\n",
              "          2060,   890, 20936,   286,   257,  2276,   966,    13,   632,  1244,\n",
              "          6901,   257,  1295,    11,  2095,    11,   393,  1429,    26,  6664,\n",
              "           378,   257,  2168,   286,  2995,    26,  8996,   393,  6273,   734,\n",
              "           393,   517,  1243,    26, 36509,  3709,   656,  9376,    26,   393,\n",
              "          6901,  5640,   290,  3048,    13, 22250,   286,   262,  1611,   286,\n",
              "          1321,   484,  3994,    11,   477, 23549,  2648,  1728,  9695,    13,\n",
              "          1881,   286,   262,   749,  1593,   286,   777,   318,   257,  7243,\n",
              "          6827,    13,   198]])"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# v. Sentence BERT"
      ],
      "metadata": {
        "id": "H5vj-b77mniI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#importing sentencebert model\n",
        "\n",
        "bert = hub.KerasLayer(\"https://tfhub.dev/google/nnlm-en-dim128/2\")\n",
        "embeddings=bert(sentences)\n",
        "print(embeddings)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7YslDVdkmrr3",
        "outputId": "c2182ea5-75ef-4bda-c339-d7cb3fc23347"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tensor(\"keras_layer_2/StatefulPartitionedCall:0\", shape=(None, 128), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"shape=\",embeddings[0].shape)\n",
        "#each sentence is converted into vector having 128 values\n",
        "print(\"The sentence in the paragraph: \",sentences[0],\"\\n is converted into vector  as : \\n{}\".format(embeddings[0]))\n",
        "shape= (128,)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yn1tn1srmuov",
        "outputId": "2e5035e2-f75b-40f9-bea6-3acf034fd41e"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape= (128,)\n",
            "The sentence in the paragraph:  A paragraph is a series of sentences that are organized and coherent, and are all related to a single topic. \n",
            " is converted into vector  as : \n",
            "Tensor(\"strided_slice_13:0\", shape=(128,), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Find named entities (NER) for the above paragraph?"
      ],
      "metadata": {
        "id": "0SuiotvLm2rg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from spacy import displacy\n",
        "ner=spacy.load('en_core_web_sm')\n",
        "res=ner(para)\n",
        "\n",
        "for word in res.ents:\n",
        "  print(word.text,word.label_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "msqfjaGgm4Sv",
        "outputId": "689d7929-bf5d-4b07-d3b7-35dd41fde973"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "two CARDINAL\n",
            "One CARDINAL\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spacy.explain('GPE')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "-QaMxpf1m9On",
        "outputId": "1bbeddf0-072a-4d7d-8b87-7c45cc7489be"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Countries, cities, states'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#better visualisation of entity recognition\n",
        "displacy.render(res,style=\"ent\",jupyter=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 227
        },
        "id": "8C9ZFN4nm_xP",
        "outputId": "70ad9aff-18f7-4b18-f1b0-4e76a6bf8bd5"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">A paragraph is a series of sentences that are organized and coherent, and are all related to a single topic. Almost every piece of writing you do that is longer than a few sentences should be organized into paragraphs. This is because paragraphs show a reader where the subdivisions of an essay begin and end, and thus help the reader see the organization of the essay and grasp its main points.</br>Paragraphs can contain many different kinds of information. A paragraph could contain a series of brief examples or a single long illustration of a general point. It might describe a place, character, or process; narrate a series of events; compare or contrast \n",
              "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    two\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">CARDINAL</span>\n",
              "</mark>\n",
              " or more things; classify items into categories; or describe causes and effects. Regardless of the kind of information they contain, all paragraphs share certain characteristics. \n",
              "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    One\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">CARDINAL</span>\n",
              "</mark>\n",
              " of the most important of these is a topic sentence.</br></div></span>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A paragraph is a series of sentences that are organized and coherent, and are all related to a single topic. Almost every piece of writing you do that is longer than a few sentences should be organized into paragraphs. This is because paragraphs show a reader where the subdivisions of an essay begin and end, and thus help the reader see the organization of the essay and grasp its main points.Paragraphs can contain many different kinds of information. A paragraph could contain a series of brief examples or a single long illustration of a general point. It might describe a place, character, or process; narrate a series of events; compare or contrast two CARDINAL or more things; classify items into categories; or describe causes and effects. Regardless of the kind of information they contain, all paragraphs share certain characteristics. One CARDINAL of the most important of these is a topic sentence.\n",
        "\n",
        "1.Find similar sentences (repeated sentences) from the above paragraph? (Cosine Similarity, use BERT to encode)"
      ],
      "metadata": {
        "id": "7m2dsCIsnGVX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install sentence_transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9koP9f4vnUtI",
        "outputId": "826bb5c3-4def-44cc-946e-973f35206a57"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: sentence_transformers in /usr/local/lib/python3.7/dist-packages (2.2.2)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (4.23.1)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (1.12.1+cu113)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (1.7.3)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (3.7)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (0.13.1+cu113)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (1.21.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (4.64.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (0.10.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (0.1.97)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (1.0.2)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (5.0.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (3.8.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (4.1.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (2.23.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (21.3)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.9->huggingface-hub>=0.4.0->sentence_transformers) (3.0.9)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (0.13.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (2022.6.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->huggingface-hub>=0.4.0->sentence_transformers) (3.9.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk->sentence_transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk->sentence_transformers) (1.2.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (2022.9.24)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (3.0.4)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sentence_transformers) (3.1.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->sentence_transformers) (7.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "sbert_model = SentenceTransformer('bert-base-nli-mean-tokens')"
      ],
      "metadata": {
        "id": "jjzJcfgXnOmA"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.Explain POS tagging with HMM? Tag POS for the above paragraph?\n",
        "POS Tagging with HMM\n",
        "\n",
        "Parts of Speech Tagging (POS): It is a process of converting a sentence to forms – list of words, list of tuples (where each tuple is having a form (word, tag)). The tag in case of is a part-of-speech tag, and signifies whether the word is a noun, adjective, verb, and so on. reading a sentence and being able to identify what words act as nouns, pronouns, verbs, adverbs, and so on. All these are referred to as the part of speech tags. According to Wikipedia, part-of-speech tagging (POS tagging or PoS tagging or POST), also called grammatical tagging or word-category disambiguation, is the process of marking up a word in a text as corresponding to a particular part of speech, based on both its definition and its context i.e. its relationship with adjacent and related words in a phrase, sentence, or paragraph.\n",
        "\n",
        "Getting Started\n",
        "\n",
        "Default tagging is a basic step for the part-of-speech tagging. It is performed using the DefaultTagger class. The DefaultTagger class takes ‘tag’ as a single argument. NN is the tag for a singular noun. DefaultTagger is most useful when it gets to work with most common part-of-speech tag. that’s why a noun tag is recommended.\n",
        "\n",
        "Getting Started\n",
        "\n",
        "POS tagging with Hidden Markov Model HMM (Hidden Markov Model) is a Stochastic technique for POS tagging. Hidden Markov models are known for their applications to reinforcement learning and temporal pattern recognition such as speech, handwriting, gesture recognition, musical score following, partial discharges, and bioinformatics. Let us consider an example proposed by Dr.Luis Serrano and find out how HMM selects an appropriate tag sequence for a sentence.\n",
        "\n",
        "Getting Started\n",
        "\n",
        "In this example, we consider only 3 POS tags that are noun, model and verb. Let the sentence “ Ted will spot Will ” be tagged as noun, model, verb and a noun and to calculate the probability associated with this particular sequence of tags we require their Transition probability and Emission probability.\n",
        "\n",
        "The transition probability is the likelihood of a particular sequence for example, how likely is that a noun is followed by a model and a model by a verb and a verb by a noun. This probability is known as Transition probability. It should be high for a particular sequence to be correct.\n",
        "\n",
        "Now, what is the probability that the word Ted is a noun, will is a model, spot is a verb and Will is a noun. These sets of probabilities are Emission probabilities and should be high for our tagging to be likely.\n",
        "\n",
        "Let us calculate the above two probabilities for the set of sentences below\n",
        "\n",
        "Mary Jane can see Will Spot will see Mary Will Jane spot Mary? Mary will pat Spot Note that Mary Jane, Spot, and Will are all names.\n",
        "\n",
        "In the above figure, we can see that the < S > tag is followed by the N tag three times, thus the first entry is 3.The model tag follows the < S > just once, thus the second entry is 1. In a similar manner, the rest of the table is filled.\n",
        "\n",
        "Next, we divide each term in a row of the table by the total number of co-occurrences of the tag in consideration, for example, The Model tag is followed by any other tags four times (in total) as shown below, thus we divide each element in the third row by four.\n",
        "\n",
        "the table is refined as below:\n",
        "\n",
        "Getting Started\n",
        "\n",
        "Calculating the product of these terms we get,\n",
        "\n",
        "3/41/93/91/43/41/414/94/9=0.00025720164\n",
        "\n",
        "For our example, keeping into consideration just three POS tags we have mentioned, 81 different combinations of tags can be formed. In this case, calculating the probabilities of all 81 combinations seems achievable. But when the task is to tag a larger sentence and all the POS tags in the Penn Treebank project are taken into consideration, the number of possible combinations grows exponentially and this task seems impossible to achieve. Now let us visualize these 81 combinations as paths and using the transition and emission probability mark each vertex and edge as shown below.\n",
        "\n",
        "Getting Started\n",
        "\n",
        "Now there are only two paths that lead to the end, let us calculate the probability associated with each path.\n",
        "\n",
        "s→N→M→N→N→ E =3/41/93/91/41/42/91/94/94/9=0.00000846754\n",
        "\n",
        "s→N→M→N→V→E=3/41/93/91/43/41/414/94/9=0.00025720164\n",
        "\n",
        "Clearly, the probability of the second sequence is much higher and hence the HMM is going to tag each word in the sentence according to this sequence."
      ],
      "metadata": {
        "id": "7ZEMTlMsninX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# POS Tagging for Above Given Paragraph\n",
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "stop_words = set(stopwords.words('english'))\n",
        " \n",
        "txt = \"A paragraph is a series of sentences that are organized and coherent, and are all related to a single topic.\" \\\n",
        "    \"Almost every piece of writing you do that is longer than a few sentences should be organized into paragraphs.\" \\\n",
        "    \"This is because paragraphs show a reader where the subdivisions of an essay begin and end, and thus help the reader see the organization of the essay and grasp its main points.\" \\\n",
        "    \"Paragraphs can contain many different kinds of information.\" \\\n",
        "    \"A paragraph could contain a series of brief examples or a single long illustration of a general point.\" \\\n",
        "    \"It might describe a place, character, or process; narrate a series of events; compare or contrast two or more things; classify items into categories; or describe causes and effects.\" \\\n",
        "    \"Regardless of the kind of information they contain, all paragraphs share certain characteristics.\" \\\n",
        "    \"One of the most important of these is a topic sentence.\"\n",
        " \n",
        "# sent_tokenize is one of instances of\n",
        "# PunktSentenceTokenizer from the nltk.tokenize.punkt module\n",
        " \n",
        "tokenized = sent_tokenize(txt)\n",
        "for i in tokenized:\n",
        "     \n",
        "    # Word tokenizers is used to find the words\n",
        "    # and punctuation in a string\n",
        "    wordsList = nltk.word_tokenize(i)\n",
        " \n",
        "    # removing stop words from wordList\n",
        "    wordsList = [w for w in wordsList if not w in stop_words]\n",
        " \n",
        "    #  Using a Tagger. Which is part-of-speech\n",
        "    # tagger or POS-tagger.\n",
        "    tagged = nltk.pos_tag(wordsList)\n",
        " \n",
        "    print(tagged)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pE_-Vj26nlgv",
        "outputId": "a81b2194-9cd9-4f9a-c3fe-dff3415aef89"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('A', 'DT'), ('paragraph', 'NN'), ('series', 'NN'), ('sentences', 'NNS'), ('organized', 'VBN'), ('coherent', 'NN'), (',', ','), ('related', 'VBN'), ('single', 'JJ'), ('topic.Almost', 'NN'), ('every', 'DT'), ('piece', 'NN'), ('writing', 'VBG'), ('longer', 'JJR'), ('sentences', 'NNS'), ('organized', 'VBD'), ('paragraphs.This', 'NN'), ('paragraphs', 'NN'), ('show', 'NN'), ('reader', 'NN'), ('subdivisions', 'NNS'), ('essay', 'VBP'), ('begin', 'JJ'), ('end', 'NN'), (',', ','), ('thus', 'RB'), ('help', 'NN'), ('reader', 'VB'), ('see', 'VB'), ('organization', 'NN'), ('essay', 'VB'), ('grasp', 'NN'), ('main', 'JJ'), ('points.Paragraphs', 'NN'), ('contain', 'VBP'), ('many', 'JJ'), ('different', 'JJ'), ('kinds', 'NNS'), ('information.A', 'VBP'), ('paragraph', 'NN'), ('could', 'MD'), ('contain', 'VB'), ('series', 'NN'), ('brief', 'NN'), ('examples', 'VBZ'), ('single', 'JJ'), ('long', 'JJ'), ('illustration', 'NN'), ('general', 'JJ'), ('point.It', 'NN'), ('might', 'MD'), ('describe', 'VB'), ('place', 'NN'), (',', ','), ('character', 'NN'), (',', ','), ('process', 'NN'), (';', ':'), ('narrate', 'JJ'), ('series', 'NN'), ('events', 'NNS'), (';', ':'), ('compare', 'VB'), ('contrast', 'NN'), ('two', 'CD'), ('things', 'NNS'), (';', ':'), ('classify', 'VB'), ('items', 'NNS'), ('categories', 'NNS'), (';', ':'), ('describe', 'VB'), ('causes', 'VBZ'), ('effects.Regardless', 'JJ'), ('kind', 'NN'), ('information', 'NN'), ('contain', 'NN'), (',', ','), ('paragraphs', 'JJ'), ('share', 'NN'), ('certain', 'JJ'), ('characteristics.One', 'NN'), ('important', 'JJ'), ('topic', 'NN'), ('sentence', 'NN'), ('.', '.')]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        }
      ]
    }
  ]
}